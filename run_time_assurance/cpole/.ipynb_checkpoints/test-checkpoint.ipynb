{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19f998b1-6e51-4fc3-95f3-8b9af2c9b62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  numpy as np\n",
    "np.random.seed(1)           # set random seed for repeatability\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import json\n",
    "import copy\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def single_relu(x):\n",
    "    if x < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "relu = np.vectorize(single_relu)\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, input_size = 4, hidden_size = 32, output_size = 2, std = 0.5, mute_rate = 0.2, strategy=None,continuous=False):\n",
    "        self.std = std                  # standard deviation\n",
    "        self.reward = 0                 # Cumulative total reward during an episode\n",
    "        self.mute_rate = mute_rate      # Mutation rate\n",
    "        self.cont = continuous\n",
    "        self.strategy=strategy\n",
    "        \n",
    "        # TODO: maybe the biases shouldn't all be initalized to .1??\n",
    "        self.b1 = np.ones(hidden_size,)         \n",
    "        self.w1 = np.random.randn(input_size, hidden_size)\n",
    "\n",
    "        self.b2 = np.random.randn(output_size,)\n",
    "        self.w2 = np.random.randn(hidden_size, output_size)\n",
    "\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Generate an action from a given state (or observation). Current implementation uses relu activation function.\n",
    "        \"\"\"\n",
    "        if self.strategy == \"action_masking\":\n",
    "             # Extract the available actions tensor from the observation.\n",
    "            action_mask = state[\"action_mask\"]\n",
    "\n",
    "            # Compute the unmasked logits.\n",
    "            l1 = relu(state[\"actual_obs\"].dot(self.w1) + self.b1)\n",
    "            logits = l1.dot(self.w2) + self.b2\n",
    "            \n",
    "            # Convert action_mask into a [0.0 || -inf]-type mask.\n",
    "            inf_mask = tf.maximum(tf.math.log(action_mask), tf.float32.min)\n",
    "            \n",
    "            # Return masked logits.\n",
    "            return np.argmax(logits + inf_mask)\n",
    "        else:\n",
    "            l1 = relu(state.dot(self.w1) + self.b1)\n",
    "            output = l1.dot(self.w2) + self.b2\n",
    "            \n",
    "            if self.cont:\n",
    "                return np.tanh(output)\n",
    "    \n",
    "            # check that this returns either -1 or +1\n",
    "            return np.argmax(output)\n",
    "        \n",
    "\n",
    "\n",
    "    def mutate(self):\n",
    "        \"\"\"\n",
    "        Mutate weights and biases for an offspring of selected agents.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        New weights and biases.\n",
    "        \"\"\"\n",
    "        # TODO: find in_l, hl, and out_l at one time instead of continually calling the function\n",
    "\n",
    "        # generate array of size inxhl\n",
    "        rnd_w1 = np.random.uniform(0, 1, [self.w1.shape[0], self.w1.shape[1]])\n",
    "        rnd_w2 = np.random.uniform(0, 1, [self.w2.shape[0], self.w2.shape[1]])\n",
    "\n",
    "        # TODO: validate changes to stay within valid number range?\n",
    "        chck_rnd1 = np.where(rnd_w1 < self.mute_rate, np.random.uniform(-self.std, self.std, [self.w1.shape[0], self.w1.shape[1]]), 0)\n",
    "        chck_rnd2 = np.where(rnd_w2 < self.mute_rate, np.random.uniform(-self.std, self.std, [self.w2.shape[0], self.w2.shape[1]]), 0)\n",
    "\n",
    "        # updates weights and biases\n",
    "        self.w1 += chck_rnd1\n",
    "        self.w2 += chck_rnd2\n",
    "\n",
    "        self.b1 += np.random.uniform(0, self.std, self.w1.shape[1])\n",
    "        self.b2 += np.random.uniform(0, self.std, self.w2.shape[1])\n",
    "\n",
    "\n",
    "    def update_reward(self, reward):\n",
    "        \"\"\" Update the cumulative sum of rewards with feedback from the environment \"\"\"\n",
    "        self.reward += reward\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Reset the cumulative sum of rewards \"\"\"\n",
    "        self.reward = 0\n",
    "\n",
    "    def save(self, name):\n",
    "        \"\"\" Save agent information to a json file \"\"\"\n",
    "        output_dict = {\n",
    "            'w1': list(self.w1.tolist()),\n",
    "            'b1': list(self.b1.tolist()),\n",
    "            'w2': list(self.w2.tolist()),\n",
    "            'b2': list(self.b2.tolist())\n",
    "        }\n",
    "        with open(name+'.json', 'w') as fp:\n",
    "            json.dump(output_dict, fp)\n",
    "\n",
    "    def load(self, name):\n",
    "        \"\"\" Load an agent from a json file \"\"\"\n",
    "        f = open(name)\n",
    "        info = json.load(f)\n",
    "        self.w1 = info['w1']\n",
    "        self.b1 = info['b1']\n",
    "        self.w2 = info['w2']\n",
    "        self.b2 = info['b2']\n",
    "        f.close()\n",
    "\n",
    "\n",
    "\n",
    "class Generation:\n",
    "\n",
    "    def __init__(self, env, continuous=False, n=20, std=.1, g=100, solved_score=190, e_rate=0.2, mute_rate=0.2, hidd_l=32, strategy=None):\n",
    "        assert (env != None), \"Must include an environment\"\n",
    "        # Parameters\n",
    "        self.generations = g      # number of generations\n",
    "        self.elite_rate = e_rate  # elitism rate\n",
    "        self.std = std            # standard deviation\n",
    "        self.n = n                # number of individuals in a population\n",
    "        self.best_fitness = []      # record best fitness across generations\n",
    "        self.avg_fitness = []       # record average fitness across generations\n",
    "        self.fitness_history = []   # record fitness history across generations\n",
    "        self.solved_score = solved_score\n",
    "        self.set_elite_num()\n",
    "        self.mute_rate = mute_rate\n",
    "        self.cont = continuous\n",
    "        self.running_total = 0\n",
    "        self.strategy = strategy\n",
    "\n",
    "        # Environment Info\n",
    "        self.env = env\n",
    "        if continuous:\n",
    "            self.action_dim = env.action_space.shape[0]\n",
    "        else:\n",
    "            self.action_dim = env.action_space.n\n",
    "        if strategy == \"action_masking\":\n",
    "            self.state_dim = env.observation_space[\"actual_obs\"].shape[0]\n",
    "        self.hidd_l = hidd_l\n",
    "\n",
    "        # Population\n",
    "        self.agents = [Agent(input_size=self.state_dim, hidden_size=hidd_l, output_size=self.action_dim, \n",
    "                             std=std, mute_rate=mute_rate, strategy=self.strategy,continuous=self.cont) for _ in range(n)]\n",
    "        self.ag_rewards = np.empty(self.n)  # stores rewards for each individual in this generation\n",
    "        self.best_agent = Agent(input_size=self.state_dim, hidden_size=hidd_l, output_size=self.action_dim, \n",
    "                                std=std, mute_rate=mute_rate, strategy=self.strategy, continuous=self.cont)\n",
    "        self.solved_score = solved_score\n",
    "        \n",
    "        \n",
    "    def set_elite_num(self):\n",
    "        \"\"\" \n",
    "        Set the number of elite individuals for each generation\n",
    "        \"\"\"\n",
    "        self.elite_num = int(round(self.n * self.elite_rate))\n",
    "        \n",
    "        if self.elite_num%2 !=0:\n",
    "            self.elite_num += 1\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" \n",
    "        Reset reward values for each agent in the population \n",
    "        \"\"\"\n",
    "        for agent in self.agents:\n",
    "            agent.reset()\n",
    "        self.ag_rewards = np.empty(self.n)\n",
    "\n",
    "\n",
    "    def crossover(self, p1, p2):\n",
    "        \"\"\"\n",
    "        Crossover and two selected parents\n",
    "        \"\"\"\n",
    "        in_l = p1.w1.shape[0]\n",
    "        hl = p1.w1.shape[1]\n",
    "        out_l =p1.w2.shape[1]\n",
    "            \n",
    "        # # Two Point Crossover\n",
    "        rnd_1 = np.random.randint(hl+1)\n",
    "        rnd_2 = np.random.randint(rnd_1, hl+1)\n",
    "        \n",
    "        w1 = np.hstack((p1.w1[:, :rnd_1], p2.w1[:, rnd_1:rnd_2], p1.w1[:, rnd_2:]))\n",
    "        w2 = np.hstack((p1.w2[:, :rnd_1], p2.w2[:, rnd_1:rnd_2], p1.w2[:, rnd_2:]))\n",
    "        \n",
    "        b1 = np.concatenate((p1.b1[:rnd_1], p2.b1[rnd_1:rnd_2], p1.b1[rnd_2:]))\n",
    "        b2 = np.concatenate((p1.b2[:rnd_1], p2.b2[rnd_1:]))\n",
    "\n",
    "        agent = Agent(input_size=self.state_dim, hidden_size=self.hidd_l, output_size=self.action_dim, \n",
    "                      std=self.std, mute_rate=self.mute_rate, strategy=self.strategy, continuous=self.cont)\n",
    "        agent.w1 = w1\n",
    "        agent.b1 = b1\n",
    "        agent.w2 = w2\n",
    "        agent.b2 = b2\n",
    "            \n",
    "        return agent\n",
    "            \n",
    "    \n",
    "    def TournamentSelect(self):\n",
    "        \"\"\" \n",
    "        Selection of agents using the tournament selection \n",
    "        \"\"\"\n",
    "        new_gen = []\n",
    "        #\n",
    "        # elitism\n",
    "        #\n",
    "        sorted_rewards = np.argsort(-self.ag_rewards)\n",
    "        for i in range(self.elite_num):\n",
    "            new_gen.append(self.agents[sorted_rewards[i]])\n",
    "        #\n",
    "        # tournament selection\n",
    "        #   \n",
    "        while len(new_gen) < self.n:\n",
    "            i1, i2, i3, i4 = np.random.randint(0, self.n, 4)\n",
    "            if self.agents[i1].reward > self.agents[i2].reward:\n",
    "                p1 = self.agents[i1]\n",
    "            else:\n",
    "                p1 = self.agents[i2]\n",
    "                \n",
    "            if self.agents[i3].reward > self.agents[i4].reward:\n",
    "                p2 = self.agents[i3]\n",
    "            else:\n",
    "                p2 = self.agents[i4]\n",
    "            \n",
    "            new_gen.append(self.crossover(p1, p2))\n",
    "            \n",
    "        return new_gen\n",
    "\n",
    "\n",
    "\n",
    "    def calc_fitness(self):\n",
    "        \"\"\" \n",
    "        Calculate fitness by rolling out each agent in the cartpole environment.\n",
    "        Fitness is the cumulative total reward during the rollout.\n",
    "        \"\"\"\n",
    "        for i in range(self.n):\n",
    "            agent = self.agents[i]\n",
    "            obs = self.env.reset()\n",
    "            for k in range(500):\n",
    "                action = agent.get_action(obs)\n",
    "                obs, reward, done, _ = self.env.step(action)\n",
    "                agent.update_reward(reward)\n",
    "                if done or k == 499:\n",
    "                    self.ag_rewards[i] = agent.reward\n",
    "                    break\n",
    "\n",
    "\n",
    "    def simulate(self):\n",
    "        \"\"\" \n",
    "        Simulate the evolution of the population for n generataions \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Best NN after g generations\n",
    "        \"\"\"\n",
    "        for k in range(self.generations):\n",
    "            #\n",
    "            # Reset each agent's reward value\n",
    "            #\n",
    "            self.reset()\n",
    "            #\n",
    "            # Calculate the fitness score for each agent in the population\n",
    "            #\n",
    "            self.calc_fitness()\n",
    "          \n",
    "            new_gen = self.TournamentSelect()  # *comment out if using RouletteWheel\n",
    "  \n",
    "            for agent in new_gen:\n",
    "                agent.mutate()\n",
    "            #\n",
    "            # Record population stats\n",
    "            #\n",
    "            self.best_fitness.append(max(self.ag_rewards))\n",
    "            self.avg_fitness.append(np.mean(self.ag_rewards))\n",
    "            #\n",
    "            # Print best fitness value every 50 generations\n",
    "            #\n",
    "            if k%10 == 0 or k==self.generations-1:\n",
    "                print(f\"Generation number:{k} | Best Fitness Value: {self.best_fitness[-1]} | Average Fitness: {self.avg_fitness[-1]}\")\n",
    "            #\n",
    "            # Stopping Criteria\n",
    "            #\n",
    "            if np.mean(self.best_fitness[-50:]) >= self.solved_score and k > 50:\n",
    "                print('Solved score threshold reached. Stopping training ...')\n",
    "                break\n",
    "            #\n",
    "            # Update new generation\n",
    "            #\n",
    "            start = time.time()\n",
    "            self.agents = new_gen\n",
    "            end = time.time()\n",
    "            \n",
    "        # Calculate final reward values of last generation and set best agent\n",
    "        self.reset()\n",
    "        self.calc_fitness()\n",
    "        self.fitness_history.append(self.ag_rewards)\n",
    "        self.best_fitness.append(max(self.ag_rewards))\n",
    "        self.avg_fitness.append(np.mean(self.ag_rewards))\n",
    "        index = np.argmax(self.ag_rewards)\n",
    "        self.best_agent = copy.deepcopy(self.agents[index])\n",
    "        \n",
    "\n",
    "    def rollout(self, render=False):\n",
    "        \"\"\" \n",
    "        Rollout the best agent \n",
    "        \"\"\"\n",
    "        obs = self.env.reset()\n",
    "        r = 0\n",
    "        start = time.time()\n",
    "        v = 0\n",
    "        \n",
    "        while True:\n",
    "            if render:\n",
    "                self.env.render()\n",
    "            pos, pos_dot, th, theta_dot = obs\n",
    "            if pos > self.x_threshold or pos < -self.x_threshold or th > self.theta_threshold or th < -self.theta_threshold:\n",
    "                v = 1\n",
    "                break\n",
    "            action = self.best_agent.get_action(obs)\n",
    "            obs, reward, done, _ = self.env.step(action)\n",
    "            r += reward\n",
    "            if done:\n",
    "                break\n",
    "        end = time.time()\n",
    "        \n",
    "        return r, abs(end-start), v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eba39555-366c-4932-bc30-df273cb819b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.custom_cpole import CartPole\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64643109-7afc-4fd9-9009-77ce2ef355a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation number:0 | Best Fitness Value: 200.0 | Average Fitness: 34.73\n",
      "Generation number:10 | Best Fitness Value: 200.0 | Average Fitness: 190.73\n",
      "Generation number:20 | Best Fitness Value: 200.0 | Average Fitness: 197.31\n",
      "Generation number:30 | Best Fitness Value: 200.0 | Average Fitness: 194.72\n",
      "Generation number:40 | Best Fitness Value: 200.0 | Average Fitness: 200.0\n",
      "Generation number:50 | Best Fitness Value: 200.0 | Average Fitness: 196.54\n",
      "Solved score threshold reached. Stopping training ...\n"
     ]
    }
   ],
   "source": [
    "env = CartPole(env_config={\"use_action_masking\": True})\n",
    "# env2 = gym.make(\"CartPole-v0\")\n",
    "# print(env2.observation_space.shape[0])\n",
    "\n",
    "# print(env.observation_space[\"actual_obs\"].shape[0])\n",
    "\n",
    "agent = Generation(env=env, n=100, g=500, solved_score = 190, e_rate=0.2, mute_rate=0.3, hidd_l=16, strategy=\"action_masking\")\n",
    "agent.simulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d6e8e2-22e6-4ce5-b76b-e1a64026ba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll(agent, num_rollouts, env_config={}, render=False):\n",
    "    action_masking = True if env_config[\"use_action_masking\"] == True else False\n",
    "    env = CartPole(env_config)\n",
    "    ep_rewards = []\n",
    "    v_total = 0\n",
    "    v_eps = 0\n",
    "    \n",
    "    for _ in range(num_rollouts):\n",
    "        safe = True\n",
    "        steps = 0\n",
    "        r = 0\n",
    "        obs = env.reset()\n",
    "        while True:\n",
    "            if render:\n",
    "                env.render()\n",
    "            if action_masking:\n",
    "                pos, pos_dot, th, theta_dot = obs[\"actual_obs\"]\n",
    "            else:\n",
    "                pos, pos_dot, th, theta_dot = obs\n",
    "            if pos >= 1.5 or pos <= -1.5 or th >= 0.1 or th <= -0.10:\n",
    "                if safe:\n",
    "                    v_eps += 1\n",
    "                    safe = False\n",
    "                v_total += 1\n",
    "                \n",
    "            action = agent.get_action(obs)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            r += reward\n",
    "            steps += 1\n",
    "            if done:\n",
    "                ep_rewards.append(r)\n",
    "                break\n",
    "    \n",
    "    return np.mean(ep_rewards), v_total, v_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e09a9a-81c4-4833-9e28-c6037bf9f1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "v = 0\n",
    "for _ in range(100):\n",
    "    r, t, v = agent.rollout()\n",
    "    scores.append(r)\n",
    "    v += 1\n",
    "    \n",
    "print(\"Scores: \", np.mean(scores))\n",
    "print(\"Violations: \", v"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_rta",
   "language": "python",
   "name": "rl_rta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
